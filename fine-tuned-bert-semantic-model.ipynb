{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9045358,"sourceType":"datasetVersion","datasetId":5453402}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch scikit-learn pandas\n","metadata":{"id":"LdvDIJFnbBru","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\ndata_dir = '/kaggle/input/imdb15/aclImdb'\n\n\n# Define the path to the dataset directory\ntrain_data_dir = '/kaggle/input/imdb15/aclImdb/train'\ntest_data_dir = '/kaggle/input/imdb15/aclImdb/test'\n\n# Load the data\ndef load_data(data_dir):\n    data = {'review': [], 'sentiment': []}\n    for label in ['pos', 'neg']:\n        labeled_dir = os.path.join(data_dir, label)\n        for review_file in os.listdir(labeled_dir):\n            if review_file.endswith('.txt'):\n                with open(os.path.join(labeled_dir, review_file), 'r', encoding='utf-8') as file:\n                    review_text = file.read()\n                    data['review'].append(review_text)\n                    data['sentiment'].append(1 if label == 'pos' else 0)\n    return pd.DataFrame(data)\n\n# Load training data\ntrain_data = load_data(train_data_dir)\nprint(train_data)\n# Load test data\ntest_data = load_data(test_data_dir)\nprint(test_data)\n# Combine train and test data if needed, or use them separately\nimdb_data = pd.concat([train_data, test_data])\n\nimdb_data = imdb_data.sample(n=10000, random_state=42)  # 10,000 samples\n# imdb_data = imdb_data.sample(frac=0.1, random_state=42) \n\n# # Zip the dataset directory\n# folder_to_zip = 'aclImdb'  # Directory to zip\n# zip_file_name = 'aclImdb.zip'\n# shutil.make_archive(zip_file_name.replace('.zip', ''), 'zip', folder_to_zip)\n\n# # Download the zip file\n# files.download(zip_file_name)\n","metadata":{"id":"wW5WUdhK91Yv","execution":{"iopub.status.busy":"2024-07-27T17:33:36.076025Z","iopub.execute_input":"2024-07-27T17:33:36.076681Z","iopub.status.idle":"2024-07-27T17:34:02.330477Z","shell.execute_reply.started":"2024-07-27T17:33:36.076648Z","shell.execute_reply":"2024-07-27T17:34:02.329395Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"                                                  review  sentiment\n0      This was one of those wonderful rare moments i...          1\n1      Have you seen The Graduate? It was hailed as t...          1\n2      I don't watch a lot of TV, except for The Offi...          1\n3      Kubrick again puts on display his stunning abi...          1\n4      First of all, I liked very much the central id...          1\n...                                                  ...        ...\n24995  The first hour of the movie was boring as hell...          0\n24996  A fun concept, but poorly executed. Except for...          0\n24997  I honestly don't understand how tripe like thi...          0\n24998  This remake of the 1962 orginal film'o the boo...          0\n24999  La Sanguisuga Conduce la Danza, or The Bloodsu...          0\n\n[25000 rows x 2 columns]\n                                                  review  sentiment\n0      I've Seen The Beginning Of The Muppet Movie, B...          1\n1      If it had been made 2 years later it would hav...          1\n2      Very good \"Precoder\" starring Dick Barthelmess...          1\n3      A young man discovers that life is precious af...          1\n4      I'm always surprised, given that the famous ti...          1\n...                                                  ...        ...\n24995  This is one of those inoffensive and mildly en...          0\n24996  When people say children are annoying u think ...          0\n24997  OK, I don't want to upset anyone who enjoyed t...          0\n24998  Words can scarcely describe this movie. Loaded...          0\n24999  I watched this movie last night, i'm a huge fa...          0\n\n[25000 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"imdb_data","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"c5CBxDSGJBS2","outputId":"95cff8ca-cd13-406d-d849-cf6ee879a295","execution":{"iopub.status.busy":"2024-07-27T17:34:02.332064Z","iopub.execute_input":"2024-07-27T17:34:02.332357Z","iopub.status.idle":"2024-07-27T17:34:02.344017Z","shell.execute_reply.started":"2024-07-27T17:34:02.332333Z","shell.execute_reply":"2024-07-27T17:34:02.342881Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"                                                  review  sentiment\n8553   For us, an Abbott and Costello movie is someth...          1\n9427   This one and \"Her Pilgrim Soul\" are two of my ...          1\n199    Spectacular Horror movie that will give you th...          1\n12447  I don't think most of us would tend to apply t...          1\n14489  I would like to say something different about ...          0\n...                                                  ...        ...\n3567   Probably this is the best film of Clint Eastwo...          1\n79     Reading some of the other comments, I must agr...          1\n18707  A young man kills a young woman for no reason....          0\n15200  I think its time for Seagal to go quietly into...          0\n5857   I first saw this film when it was transmitted ...          1\n\n[10000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8553</th>\n      <td>For us, an Abbott and Costello movie is someth...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9427</th>\n      <td>This one and \"Her Pilgrim Soul\" are two of my ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>Spectacular Horror movie that will give you th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12447</th>\n      <td>I don't think most of us would tend to apply t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14489</th>\n      <td>I would like to say something different about ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3567</th>\n      <td>Probably this is the best film of Clint Eastwo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Reading some of the other comments, I must agr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18707</th>\n      <td>A young man kills a young woman for no reason....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15200</th>\n      <td>I think its time for Seagal to go quietly into...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5857</th>\n      <td>I first saw this film when it was transmitted ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"zk4BCD9tDtwQ","execution":{"iopub.status.busy":"2024-07-27T18:16:16.813549Z","iopub.execute_input":"2024-07-27T18:16:16.813900Z","iopub.status.idle":"2024-07-27T18:16:16.825991Z","shell.execute_reply.started":"2024-07-27T18:16:16.813872Z","shell.execute_reply":"2024-07-27T18:16:16.825010Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"train_data = load_data(train_data_dir)\nprint(\"Training Data:\")\nprint(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-27T17:34:13.411722Z","iopub.execute_input":"2024-07-27T17:34:13.412310Z","iopub.status.idle":"2024-07-27T17:34:25.987587Z","shell.execute_reply.started":"2024-07-27T17:34:13.412276Z","shell.execute_reply":"2024-07-27T17:34:25.986455Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Training Data:\n                                                  review  sentiment\n0      This was one of those wonderful rare moments i...          1\n1      Have you seen The Graduate? It was hailed as t...          1\n2      I don't watch a lot of TV, except for The Offi...          1\n3      Kubrick again puts on display his stunning abi...          1\n4      First of all, I liked very much the central id...          1\n...                                                  ...        ...\n24995  The first hour of the movie was boring as hell...          0\n24996  A fun concept, but poorly executed. Except for...          0\n24997  I honestly don't understand how tripe like thi...          0\n24998  This remake of the 1962 orginal film'o the boo...          0\n24999  La Sanguisuga Conduce la Danza, or The Bloodsu...          0\n\n[25000 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"imdb_data","metadata":{"execution":{"iopub.status.busy":"2024-07-27T17:35:17.222656Z","iopub.execute_input":"2024-07-27T17:35:17.222993Z","iopub.status.idle":"2024-07-27T17:35:17.234173Z","shell.execute_reply.started":"2024-07-27T17:35:17.222966Z","shell.execute_reply":"2024-07-27T17:35:17.233052Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"                                                  review  sentiment\n8553   For us, an Abbott and Costello movie is someth...          1\n9427   This one and \"Her Pilgrim Soul\" are two of my ...          1\n199    Spectacular Horror movie that will give you th...          1\n12447  I don't think most of us would tend to apply t...          1\n14489  I would like to say something different about ...          0\n...                                                  ...        ...\n3567   Probably this is the best film of Clint Eastwo...          1\n79     Reading some of the other comments, I must agr...          1\n18707  A young man kills a young woman for no reason....          0\n15200  I think its time for Seagal to go quietly into...          0\n5857   I first saw this film when it was transmitted ...          1\n\n[10000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8553</th>\n      <td>For us, an Abbott and Costello movie is someth...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9427</th>\n      <td>This one and \"Her Pilgrim Soul\" are two of my ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>Spectacular Horror movie that will give you th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12447</th>\n      <td>I don't think most of us would tend to apply t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14489</th>\n      <td>I would like to say something different about ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3567</th>\n      <td>Probably this is the best film of Clint Eastwo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Reading some of the other comments, I must agr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18707</th>\n      <td>A young man kills a young woman for no reason....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15200</th>\n      <td>I think its time for Seagal to go quietly into...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5857</th>\n      <td>I first saw this film when it was transmitted ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Prepare data for fine-tuning\ntexts = imdb_data['review']\nlabels = imdb_data['sentiment']  # 'sentiment' column contains labels (0 for negative, 1 for positive)\n\n# Use BERT tokenizer\n# tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Tokenize texts\ntokenized_texts = tokenizer(list(texts), padding=True, truncation=True, return_tensors='pt')\n\n# Convert labels to PyTorch tensor\n# labels = labels.apply(lambda x: 1 if x == 'positive' else 0)  # Convert labels to numeric values\nlabels = torch.tensor(labels.values)\nprint(labels)\n# Create TensorDataset\ndataset = TensorDataset(tokenized_texts.input_ids, tokenized_texts.attention_mask, labels)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n\ntest_size = int(0.2 * len(dataset))  # 20% for the test set\nval_size = int(0.2 * (len(dataset) - test_size))  # 20% of the remaining data for validation\ntrain_size = len(dataset) - val_size - test_size  # The rest is for training\n\n# Split dataset into train, validation, and test sets\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# DataLoader for training, validation, and test sets\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nprint(\"train_dataloader Done\")\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)\nprint(\"val_dataloader Done\")\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\nprint(\"test_dataloader Done\")\n\n# Load pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # 2 labels: positive, negative\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m0waTdCdCWc5","outputId":"fbb678be-2263-4f90-c51c-8662c1dea1a4","execution":{"iopub.status.busy":"2024-07-27T18:21:23.070407Z","iopub.execute_input":"2024-07-27T18:21:23.070825Z","iopub.status.idle":"2024-07-27T18:22:53.342524Z","shell.execute_reply.started":"2024-07-27T18:21:23.070792Z","shell.execute_reply":"2024-07-27T18:22:53.341701Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"tensor([1, 1, 1,  ..., 0, 0, 1])\ncuda\ntrain_dataloader Done\nval_dataloader Done\ntest_dataloader Done\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-27T18:50:22.124091Z","iopub.execute_input":"2024-07-27T18:50:22.124470Z","iopub.status.idle":"2024-07-27T18:50:22.278116Z","shell.execute_reply.started":"2024-07-27T18:50:22.124431Z","shell.execute_reply":"2024-07-27T18:50:22.277042Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Define the model with dropout (if not already included in your model)\nmodel =BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n# model.classifier.dropout = torch.nn.Dropout(p=0.3)  # Assuming you have a dropout layer in your model definition\n\n# Initialize the optimizer with weight decay\noptimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8, weight_decay=0.01)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 2)\n\n# Set the device to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nmodel.to(device)\n\nepochs = 2  \n\n# Early stopping parameters\nearly_stopping_patience = 2\nbest_val_loss = float('inf')\npatience_counter = 0\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    model.train()\n    total_loss = 0\n\n    for batch in train_dataloader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n\n    # Validation\n    model.eval()\n    total_val_loss = 0\n    true_labels = []\n    predictions = []\n\n    for batch in val_dataloader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n            logits = outputs.logits\n            true_labels.extend(labels.cpu().numpy())\n            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_dataloader)\n    val_accuracy = accuracy_score(true_labels, predictions)\n    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n\n    print(f\"Epoch {epoch + 1}/{epochs} - Average Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n    print(f\"Validation Precision: {val_precision:.4f}\")\n    print(f\"Validation Recall: {val_recall:.4f}\")\n    print(f\"Validation F1 Score: {val_f1:.4f}\")\n\n    # Early stopping check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        # Save the best model\n        model.save_pretrained('final_fine_tuned_bert_semantic_model')\n    else:\n        patience_counter += 1\n        if patience_counter >= early_stopping_patience:\n            print(\"Early stopping triggered\")\n            break\n\n# Save the final model\nmodel.save_pretrained('final_fine_tuned_bert_semantic_model')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T19:51:49.017929Z","iopub.execute_input":"2024-07-27T19:51:49.018868Z","iopub.status.idle":"2024-07-27T20:13:39.692346Z","shell.execute_reply.started":"2024-07-27T19:51:49.018832Z","shell.execute_reply":"2024-07-27T20:13:39.691530Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"cuda\nEpoch 1/2\nEpoch 1/2 - Average Training Loss: 0.7216 - Validation Loss: 0.2421\nValidation Accuracy: 0.9056\nValidation Precision: 0.8819\nValidation Recall: 0.9378\nValidation F1 Score: 0.9090\nEpoch 2/2\nEpoch 2/2 - Average Training Loss: 0.3262 - Validation Loss: 0.2339\nValidation Accuracy: 0.9062\nValidation Precision: 0.8884\nValidation Recall: 0.9303\nValidation F1 Score: 0.9089\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"hcEYKUyvHsqb"},"execution_count":null,"outputs":[]}]}